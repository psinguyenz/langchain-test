{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be98d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c35875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd277252",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532c600e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A curious fact about Cristiano Ronaldo is that he is the only player in history to have won league titles in England (Premier League with Manchester United), Spain (La Liga with Real Madrid), and Italy (Serie A with Juventus). This showcases his incredible adaptability and success across different top European football leagues.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d436e",
   "metadata": {},
   "source": [
    "# Use of .bind() to add arguments to a Runnable in a LCEL Chain\n",
    "- For example, we can add an argument to stop the model response when it reaches the word \"Ronaldo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4491a0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A curious fact about Cristiano '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model.bind(stop=[\"Ronaldo\"]) | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc8f14",
   "metadata": {},
   "source": [
    "# Combining LCEL Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2432f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5b4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a sentence about {politician}\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b17720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neville Chamberlain was the British Prime Minister known for his policy of appeasement toward Adolf Hitler prior to World War II.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Chamberlain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2787b",
   "metadata": {},
   "source": [
    "# Combined chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe7911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {politician} positive for Humanity?\")\n",
    "\n",
    "composed_chain = {\"politician\": chain} | historian_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c187192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, Abraham Lincoln is widely regarded as having had a positive impact on humanity. As the 16th President of the United States, he led the nation through its most challenging period, the Civil War, striving to preserve the Union. Importantly, he played a crucial role in ending slavery in the United States through the issuance of the Emancipation Proclamation in 1863 and by advocating for the passage of the 13th Amendment, which abolished slavery. His leadership helped to advance human rights and set a foundation for greater equality, making his legacy largely positive for humanity.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"politician\": \"Lincoln\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d024f",
   "metadata": {},
   "source": [
    "# A chain inside another chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea74cf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Châu Âu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"Vietnamese\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9691a",
   "metadata": {},
   "source": [
    "# LCEL chain at work in a typical RAG app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d3c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aef97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), # extract all infomation from a website\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\") # crawl only from these class\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Use hub.pull() - still works but shows deprecation warning\n",
    "# For newer version, use: from langsmith import pull_prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # use the prompt from prompt hub instead of writing your own prompt\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23898fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "c:\\Users\\admin\\anaconda3\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "c:\\Users\\admin\\anaconda3\\envs\\llmapp\\Lib\\site-packages\\chromadb\\types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps or subgoals to facilitate planning and problem-solving. Techniques like Chain of Thought (CoT) prompt models to \"think step by step,\" while methods like Tree of Thoughts explore multiple reasoning paths. It can be performed by LLMs with simple prompts, task-specific instructions, human inputs, or by using external classical planners via intermediate representations like PDDL.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\") # A few small error for the older version but still workable\n",
    "# updating the requirements.txt will fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
