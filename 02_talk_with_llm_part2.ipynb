{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7f5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ee57e",
   "metadata": {},
   "source": [
    "# Completion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f2b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b4d88",
   "metadata": {},
   "source": [
    "# Chat completion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc7a9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf2d17",
   "metadata": {},
   "source": [
    "# Prompts and Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f401e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for completions model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} sory about {topic}\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"The Kennedy family\"\n",
    ")\n",
    "\n",
    "res = llmModel.invoke(llmModelPrompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bed5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy Sr. and Rose Fitzgerald Kennedy had nine children, many of whom had children of their own, resulting in a large number of grandchildren.\n",
      "\n",
      "Joseph P. Kennedy Sr. had 29 grandchildren in total. If you'd like, I can provide more details about some of his notable grandchildren or the Kennedy family's subsequent generations.\n"
     ]
    }
   ],
   "source": [
    "# This is for chat completions model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren Joseph P. Kennedy had?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d037e",
   "metadata": {},
   "source": [
    "# Old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import SystemMessage\n",
    "# from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# chat_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=(\n",
    "#                 \"You are an Historian expert on the Kennedy family.\"\n",
    "#             )\n",
    "#         ),\n",
    "#         HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# messages = chat_template.format_messages(\n",
    "#     user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
    "# )\n",
    "\n",
    "# response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fb6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Joseph P. Kennedy Sr. and his wife Rose Fitzgerald Kennedy had nine children. Here are their names:\\n\\n1. Joseph P. Kennedy Jr. (1915–1944)\\n2. John F. Kennedy (1917–1963)\\n3. Rosemary Kennedy (1918–2005)\\n4. Kathleen Kennedy (1920–1948)\\n5. Eunice Kennedy (1921–2009)\\n6. Patricia Kennedy (1924–2006)\\n7. Robert F. Kennedy (1925–1968)\\n8. Jean Kennedy (born 1928)\\n9. Edward M. Kennedy (1932–2009)\\n\\nAs for grandchildren, the Kennedy family is quite extensive. Here are some notable grandchildren of Joseph P. Kennedy Sr.:\\n\\n- Children of John F. Kennedy and Jacqueline Kennedy Onassis:\\n  - Caroline Kennedy (born 1957)\\n  - John F. Kennedy Jr. (1960–1999)\\n\\n- Children of Robert F. Kennedy and Ethel Kennedy:\\n  - Kathleen Kennedy Townsend (born 1951)\\n  - Joseph P. Kennedy II (born 1952)\\n  - Robert F. Kennedy Jr. (born 1954)\\n  - David Kennedy (1955–1984)\\n  - Mary Kerry Kennedy (born 1959)\\n  - Christopher G. Kennedy (born 1963)\\n  - Max Kennedy (born 1965)\\n  - Douglas Kennedy (born 1967)\\n  - Rory Kennedy (born 1968)\\n\\n- Children of Edward M. Kennedy:\\n  - Kara Kennedy (1960–2011)\\n  - Edward M. Kennedy Jr. (born 1961)\\n  - Patrick J. Kennedy (born 1967)\\n\\nOther children of Joseph Sr.'s other offspring also had children, but the above list covers many of the more publicly known grandchildren. If you want information about specific branches or more detailed family trees, feel free to ask!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84cd573",
   "metadata": {},
   "source": [
    "# Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dfbcd",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae235367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chatModel\n",
    "\n",
    "res = chain.invoke({\"input\": \"How are you?\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f8a0a",
   "metadata": {},
   "source": [
    "# Langchain Expressions Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef59961",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf06ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de101564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffa007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json_chain.invoke({\"question\": \"What is the biggest contry?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e88502",
   "metadata": {},
   "source": [
    "# Optionally, you can use Pydantic to define a custom output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b532059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "951ddd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic Object with the desired output format.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e43f9afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parser referring the Pydantic Object\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Add the parser format instructions in the prompt definition.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | chatModel | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
